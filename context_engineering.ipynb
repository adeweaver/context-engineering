{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6ee7ab-2b6c-4b90-b41a-fe8e911a100a",
   "metadata": {},
   "source": [
    "# Context engineering with WRITER and Strands Agents SDK\n",
    "\n",
    "Using WRITER's Palmyra-X5 and integration with Strands Agents SDK, we're going to walk through the core components of **context engineering** and how you can create agents capable of scaling in the enterprise, mitigating things like **context rot**.\n",
    "\n",
    "## What is context engineering?\n",
    "\n",
    "**Context engineering** is the discipline of how we structure, retrieve, and manage information so that LLMs can reason effectively. It goes beyond prompts to include instructions, knowledge, tools, retrieval, memory, and governance â€” all orchestrated to ensure the model has the *right information in the right format at the right time*. As conversations grow longer, the context window becomes a critical bottleneck that can lead to:\n",
    "\n",
    "- **Context rot**: Degradation in agent performance as conversations exceed model context limits\n",
    "    - **Context Poisoning**: When a hallucination makes it into the context\n",
    "    - **Context Distraction**: When the context overwhelms the training\n",
    "    - **Context Confusion**: When superfluous context influences the response\n",
    "    - **Context Clash**: When parts of the context disagree\n",
    "\n",
    "## How context is generated\n",
    "\n",
    "As enterprises move beyond simple Q&A and into multi-turn, agentic systems that can reason and act over long time horizons, single-prompt engineering, alone, canâ€™t manage the flood of context these systems depend on:\n",
    "\n",
    "![Types of Context Management](resources/types_of_context.png)\n",
    "\n",
    "- **Instructions (the rules)**: system prompts, task instructions, few-shot examples, formatting rules, and tool descriptions\n",
    "- **Knowledge (the facts)**: documents, embeddings, domain-specific data, chat history, and long-term memory\n",
    "- **Tools (the actions)**: outputs and feedback from external systems - APIs, search engines, calculators, retrieval functions\n",
    "\n",
    "The diagram above illustrates the different approaches to context management that we'll explore in this notebook, from simple sliding windows to intelligent summarization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fe2b7-59fb-47dc-9e17-113a2ec92bc5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Setup and installation\n",
    "\n",
    "First, let's install the required dependencies for working with Strands Agents SDK and WRITER integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930c52b",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install 'strands-agents[writer]' strands-agents-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbecbf-e0b7-4cd6-9a9a-24d75b15707c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "We need to set up the WRITER API key to authenticate with the Palmyra-X5 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a748b-8dc5-4a84-aac9-abddfd1031ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"WRITER_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd573fe-b34f-4473-ba54-a1fa27b1a891",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model and agent setup\n",
    "\n",
    "Let's create a basic agent with WRITER's Palmyra-X5 model to test our setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b19c08-5249-487b-95bd-e8a25fc15a4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models.writer import WriterModel\n",
    "\n",
    "model = WriterModel(\n",
    "    model_id=\"palmyra-x5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1397be6-b064-4791-87a1-16532a775037",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Now let's test our agent with a simple question to ensure everything is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21919a2a-ffba-4abb-bd5d-edada95efd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Assistant\",\n",
    "    system_prompt=\"Reply very concisely.\",\n",
    ")\n",
    "result = agent(\"Tell me why it is important to evaluate AI agents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32796613-1ef8-4ed2-936f-a8086e84eabc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## State & history: proactive context management with persistence\n",
    "\n",
    "A central challenge in **context engineering** is deciding *how much of a conversationâ€™s history should remain in the modelâ€™s context window.*  \n",
    "Too little history, and the agent loses continuity. Too much, and it wastes tokens, increases latency, and risks â€œcontext rot.â€\n",
    "\n",
    "This section demonstrates **proactive context management** â€” a strategy that trims older messages *before* they are passed to the model.  \n",
    "By doing this early, the system ensures that only the most recent and relevant user turns are preserved, keeping context focused and performant.\n",
    "\n",
    "\n",
    "### How It Works\n",
    "\n",
    "We extend Strandsâ€™ built-in `SlidingWindowConversationManager` into a custom **`ProactiveTurnTrimmer`**, which:\n",
    "\n",
    "- **Trims proactively** â†’ Always keeps only the last *N* user turns before sending the next model call.  \n",
    "- **Preserves continuity** â†’ Maintains conversational flow without unnecessary historical baggage.  \n",
    "- **Optimizes resources** â†’ Ensures predictable token usage and model performance in long-running sessions.\n",
    "\n",
    "This differs from a *reactive* approach (which trims *after* overflow).  \n",
    "Proactive trimming is like an automatic cleanup crew â€” keeping your modelâ€™s short-term memory clean and coherent before every turn.\n",
    "\n",
    "---\n",
    "\n",
    "### FileSessionManager â€” Persistent Session Memory\n",
    "\n",
    "In parallel, we use **`FileSessionManager`** to handle **long-term persistence** of the agentâ€™s state.  \n",
    "While the `ProactiveTurnTrimmer` manages *active context*, `FileSessionManager` preserves *historical memory* between runs.\n",
    "\n",
    "#### What It Does\n",
    "- Stores all messages as structured JSON under `.strands/sessions/...`\n",
    "- Allows sessions to be reloaded or replayed later\n",
    "- Creates a lightweight local â€œmemory layerâ€ for experimentation or auditability\n",
    "\n",
    "#### Why It Matters\n",
    "Together, these components embody the **inner and outer loops** of context engineering:\n",
    "\n",
    "| Layer | Mechanism | Function | Context Principle |\n",
    "|:------|:-----------|:----------|:------------------|\n",
    "| **Inner Loop** | `ProactiveTurnTrimmer` | Manage active conversation window | *Isolate* |\n",
    "| **Outer Loop** | `FileSessionManager` | Persist full conversation to disk | *Write* |\n",
    "\n",
    "This pairing ensures that:\n",
    "- The **active context** (short-term) stays clean and relevant.\n",
    "- The **session history** (long-term) remains available for recall, analysis, or summarization later.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters in Enterprise Contexts\n",
    "\n",
    "In production-scale systems â€” customer assistants, multi-agent workflows, or regulated industries â€” proactive context management is critical for:\n",
    "\n",
    "- **Consistency** â†’ Prevent drift by constraining context size and order.  \n",
    "- **Cost-efficiency** â†’ Control token growth across multi-turn conversations.  \n",
    "- **Compliance & Auditability** â†’ Persist all interactions for review via session storage.  \n",
    "- **Scalability** â†’ Maintain stable reasoning as conversations evolve over time.\n",
    "\n",
    "> **Context engineering isnâ€™t just about prompt design â€” itâ€™s about lifecycle management.**  \n",
    "> The proactive trimming strategy governs what stays *in the modelâ€™s mind* (state/history),  \n",
    "> while session persistence governs what stays *on record* (memory).\n",
    "\n",
    "âœ… **Next Step:**  \n",
    "Weâ€™ll build on this by adding summarization â€” compressing older context while keeping recent turns verbatim, merging *windowing* with *memory abstraction* for full context lifecycle management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a2c57-215f-4cdf-91f5-570e8325c5cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from strands import Agent\n",
    "from strands.models.writer import WriterModel\n",
    "from strands.agent.conversation_manager.sliding_window_conversation_manager import SlidingWindowConversationManager\n",
    "from strands.session import FileSessionManager\n",
    "\n",
    "# --- Storage setup(local) ---\n",
    "storage_dir = pathlib.Path(\".strands/sessions\").resolve()\n",
    "\n",
    "# --- Helper: identify user messages ---\n",
    "def _is_user_msg(item: Dict[str, Any]) -> bool:\n",
    "    return item.get(\"role\") == \"user\"\n",
    "\n",
    "# --- Proactive Turn-Based Conversation Manager ---\n",
    "class ProactiveTurnTrimmer(SlidingWindowConversationManager):\n",
    "    def __init__(self, max_turns: int = 3, **kwargs: Any):\n",
    "        \"\"\"Trim proactively to the last N user turns each time.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_turns = max(1, int(max_turns))\n",
    "\n",
    "    def select_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Always trim proactively before sending to the model.\"\"\"\n",
    "        if not messages:\n",
    "            return messages\n",
    "\n",
    "        count = 0\n",
    "        start_idx = 0\n",
    "        for i in range(len(messages) - 1, -1, -1):\n",
    "            if _is_user_msg(messages[i]):\n",
    "                count += 1\n",
    "                if count == self.max_turns:\n",
    "                    start_idx = i\n",
    "                    break\n",
    "        return messages[start_idx:]\n",
    "\n",
    "# --- Setup Agent with (local) FileSessionManager ---\n",
    "session_proactive = FileSessionManager(\n",
    "    session_id=\"proactive-demo\",\n",
    "    storage_dir=storage_dir\n",
    ")\n",
    "\n",
    "model = WriterModel(model_id=\"palmyra-x5\")\n",
    "\n",
    "proactive_agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Proactive Assistant\",\n",
    "    system_prompt=\"Be concise.\",\n",
    "    conversation_manager=ProactiveTurnTrimmer(max_turns=2),\n",
    "    session_manager=session_proactive,\n",
    ")\n",
    "\n",
    "# --- Inspector Utility ---\n",
    "def show_effective_context(agent: Agent):\n",
    "    trimmed = agent.conversation_manager.select_messages(agent.messages)\n",
    "    print(f\"\\n Effective Context for {agent.name}:\")\n",
    "    for msg in trimmed:\n",
    "        print(msg)\n",
    "\n",
    "# --- Test run ---\n",
    "def test_proactive_agent():\n",
    "    print(f\"\\n=== {proactive_agent.name} ===\")\n",
    "    proactive_agent(\"How is your day?\")\n",
    "    proactive_agent(\"What did I just say?\")\n",
    "    proactive_agent(\"What model are we using?\")\n",
    "\n",
    "    # Show what the model actually \"sees\" (proactive state/history)\n",
    "    show_effective_context(proactive_agent)\n",
    "\n",
    "# Run test\n",
    "test_proactive_agent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e2f7e-8571-42e0-8b42-deb2fe988fa4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Session storage inspection\n",
    "\n",
    "To understand how Strands persists conversation data, we can inspect whatâ€™s stored on disk.  \n",
    "The `FileSessionManager` saves each message (user, assistant, tool, etc.) as a structured JSON file with metadata like timestamps, roles, and IDs.\n",
    "\n",
    "This snippet adds **observability** to session storage â€” helping you verify whatâ€™s actually persisted versus what remains in active memory.\n",
    "\n",
    "---\n",
    "\n",
    "### inspect_session() â€” observing stored history\n",
    "\n",
    "#### Purpose\n",
    "Adds **visibility** into the agentâ€™s stored state by reading session files and printing their contents.\n",
    "\n",
    "#### What it does\n",
    "- Iterates through all saved messages on disk.  \n",
    "- Extracts key details such as role (`user`, `assistant`, etc.) and text content.  \n",
    "- Displays the full chronological order of a session, providing a transparent view of the stored memory state.\n",
    "\n",
    "#### Why it matters\n",
    "Observability is a critical part of **context engineering**.  \n",
    "It allows you to debug *when* and *why* context was trimmed, summarized, or persisted â€”  \n",
    "bridging the gap between the modelâ€™s **active state** (what itâ€™s reasoning over now)  \n",
    "and its **stored history** (what it remembers across turns and sessions).\n",
    "\n",
    "> Use this utility after running a conversation to inspect your `.strands/sessions` directory  \n",
    "> and confirm that messages are being written, trimmed, or summarized as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5840ece-bc99-44bf-9da0-872607865362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "def extract_message(m: dict):\n",
    "    # Messages are nested under \"message\"\n",
    "    msg = m.get(\"message\", m)  \n",
    "\n",
    "    role = msg.get(\"role\", \"unknown\")\n",
    "\n",
    "    content = msg.get(\"content\", \"\")\n",
    "    if isinstance(content, list):\n",
    "        text = \" \".join(c.get(\"text\", \"\") for c in content if isinstance(c, dict))\n",
    "    elif isinstance(content, str):\n",
    "        text = content\n",
    "    else:\n",
    "        text = str(content)\n",
    "\n",
    "    return role, text\n",
    "\n",
    "\n",
    "def inspect_session(session_id: str):\n",
    "    base_dir = pathlib.Path(f\".strands/sessions/session_{session_id}/agents\")\n",
    "    if not base_dir.exists():\n",
    "        print(f\" No session found for {session_id}\")\n",
    "        return\n",
    "\n",
    "    for agent_dir in base_dir.iterdir():\n",
    "        print(f\"\\n Agent: {agent_dir.name}\")\n",
    "        messages_dir = agent_dir / \"messages\"\n",
    "        if not messages_dir.exists():\n",
    "            print(\"   (no messages yet)\")\n",
    "            continue\n",
    "\n",
    "        print(\" Conversation:\")\n",
    "        for msg_file in sorted(messages_dir.glob(\"message_*.json\")):\n",
    "            with open(msg_file) as f:\n",
    "                raw = json.load(f)\n",
    "            role, text = extract_message(raw)\n",
    "            print(f\"{role}: {text}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "inspect_session(\"proactive-demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb6914-b2d0-444d-a35d-ee0fe6d0552b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Context compression: summarizing conversation history\n",
    "\n",
    "After exploring proactive context trimming, the next step in **context engineering**  \n",
    "is to **compress** older conversation history without losing important information.  \n",
    "\n",
    "The `SummarizingConversationManager` introduces an automatic summarization layer â€”  \n",
    "it shortens the *oldest parts* of a conversation while preserving recent turns verbatim.  \n",
    "This bridges the gap between **short-term state management** and **long-term memory retention**.\n",
    "\n",
    "\n",
    "### How it works\n",
    "\n",
    "When the conversation grows too long, this manager:\n",
    "1. Identifies the *oldest* messages beyond a given threshold (`summary_ratio`).\n",
    "2. Uses the model itself to summarize that portion of the conversation.  \n",
    "3. Keeps the most recent few turns intact (`preserve_recent_messages`).\n",
    "4. Replaces the trimmed history with a concise summary message.\n",
    "\n",
    "This allows the agent to:\n",
    "- Maintain awareness of prior topics and facts,  \n",
    "- Reduce token usage dramatically,  \n",
    "- Preserve natural flow without overwhelming the model context window.\n",
    "\n",
    "---\n",
    "\n",
    "### Parameters used in this example\n",
    "\n",
    "| Parameter | Description |\n",
    "|------------|-------------|\n",
    "| `summary_ratio=0.3` | Summarize roughly 30% of the oldest messages when the window fills. |\n",
    "| `preserve_recent_messages=4` | Always keep the latest four messages unaltered. |\n",
    "| `summarization_system_prompt` | Custom instruction defining how to summarize. |\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "In **context engineering**, summarization is the **â€œcompressâ€** phase â€”  \n",
    "itâ€™s how we preserve meaning across long conversations while keeping the model efficient.\n",
    "\n",
    "Where the **ProactiveTurnTrimmer** focused on short-term state and history,  \n",
    "the **SummarizingConversationManager** introduces a middle-layer memory â€”  \n",
    "an abstraction that turns verbose logs into a compact, meaningful summary.\n",
    "\n",
    "> This is how context evolves from â€œraw conversation historyâ€  \n",
    "> into â€œsemantic memoryâ€ that still fits within the modelâ€™s context window.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Next step:**  \n",
    "Weâ€™ll combine both strategies â€” sliding-window trimming for state  \n",
    "and summarization for memory â€” into a single hybrid conversation manager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6c9ba-9f16-4a04-83c4-ae9e8685f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from strands import Agent\n",
    "from strands.models.writer import WriterModel\n",
    "from strands.agent.conversation_manager.summarizing_conversation_manager import SummarizingConversationManager\n",
    "from strands.session.file_session_manager import FileSessionManager\n",
    "\n",
    "# --- Storage directory for persistence ---\n",
    "storage_dir = pathlib.Path(\".strands/sessions\").resolve()\n",
    "\n",
    "# --- Setup summarizing manager ---\n",
    "# summary_ratio=0.3 means: summarize about 30% of the oldest messages\n",
    "# preserve_recent_messages=4 means: always keep at least the last 4 raw messages\n",
    "summarizing_manager = SummarizingConversationManager(\n",
    "    summary_ratio=0.3,\n",
    "    preserve_recent_messages=4,\n",
    "    summarization_system_prompt=\"Summarize the following conversation briefly but keep all important details.\"\n",
    ")\n",
    "\n",
    "# --- Session + model setup ---\n",
    "session_summarizing = FileSessionManager(\n",
    "    session_id=\"summarizing-demo\",\n",
    "    storage_dir=storage_dir\n",
    ")\n",
    "\n",
    "model = WriterModel(model_id=\"palmyra-x5\")\n",
    "\n",
    "summarizing_agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Summarizing Assistant\",\n",
    "    system_prompt=\"You are a helpful summarizing assistant.\",\n",
    "    conversation_manager=summarizing_manager,\n",
    "    session_manager=session_summarizing,\n",
    ")\n",
    "\n",
    "# --- Run a demo conversation ---\n",
    "print(\"=== Summarizing Agent Demo ===\")\n",
    "summarizing_agent(\"Hello, Iâ€™m researching space travel history.\")\n",
    "summarizing_agent(\"Tell me about the Apollo missions.\")\n",
    "summarizing_agent(\"What happened during Apollo 13?\")\n",
    "summarizing_agent(\"Summarize what weâ€™ve talked about so far.\")\n",
    "\n",
    "# --- Inspect current active context (model input after summarization) ---\n",
    "print(\"\\nâœ‚ï¸ Active Context (summarized):\")\n",
    "for msg in summarizing_agent.messages:\n",
    "    print(msg)\n",
    "\n",
    "# --- Inspect persisted session on disk ---\n",
    "base_dir = storage_dir / \"session_summarizing-demo\" / \"agents\"\n",
    "if base_dir.exists():\n",
    "    for agent_dir in base_dir.iterdir():\n",
    "        print(f\"\\n Agent: {agent_dir.name}\")\n",
    "        msgs_dir = agent_dir / \"messages\"\n",
    "        for msg_file in sorted(msgs_dir.glob(\"message_*.json\")):\n",
    "            print(msg_file.name, \"->\", open(msg_file).read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460df91c-a56d-429a-bbf0-258981c71306",
   "metadata": {},
   "source": [
    "## ðŸ§  Hybrid Context Management: Combining Trimming and Summarization\n",
    "\n",
    "So far, weâ€™ve explored two key patterns:\n",
    "1. **Proactive Trimming** â†’ Keep only the most recent turns (short-term state).  \n",
    "2. **Summarization** â†’ Compress older turns into a concise summary (long-term memory).\n",
    "\n",
    "The next evolution is to **combine both strategies** into a single, adaptive system.  \n",
    "This `HybridSummarizingTurnTrimmer` demonstrates how an agent can intelligently balance *context depth* and *context efficiency*.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "This implementation enforces two key parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|------------|-------------|\n",
    "| `context_limit` | Maximum number of user turns to keep in raw history before summarization triggers. |\n",
    "| `keep_last_n_turns` | Number of most recent turns to preserve exactly as they occurred. |\n",
    "\n",
    "When the number of user turns exceeds `context_limit`, the manager:\n",
    "1. **Counts user turns** in the current conversation.  \n",
    "2. **Summarizes everything before** the earliest of the last `keep_last_n_turns` turns.  \n",
    "3. **Injects a synthetic conversation pair**:\n",
    "   - `user`: â€œSummarize the conversation we had so far.â€  \n",
    "   - `assistant`: *(generated summary)*  \n",
    "4. **Appends the recent turns** verbatim after the summary.\n",
    "\n",
    "This creates a natural conversation flow where the agent remembers prior context through a summary â€” just as a human might recall *â€œWe already talked about that earlierâ€¦â€* â€” while keeping the most recent details intact.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters for Context Engineering\n",
    "\n",
    "The hybrid model captures the full **context lifecycle**:\n",
    "\n",
    "| Context Layer | Strategy | Component | Goal |\n",
    "|:---------------|:----------|:-----------|:------|\n",
    "| **Active Context (short-term)** | Sliding window trimming | `SlidingWindowConversationManager` | Maintain focus and coherence |\n",
    "| **Summarized Memory (mid-term)** | Context compression | `_summarize_old_history()` | Preserve meaning, reduce token load |\n",
    "| **Persistent State (long-term)** | File-based storage | `FileSessionManager` | Enable recovery, auditing, continuity |\n",
    "\n",
    "Together, these mechanisms form the backbone of **enterprise-grade context engineering**:\n",
    "- **Scalable** â†’ Handles long, multi-session dialogues gracefully.  \n",
    "- **Composable** â†’ Fits within multi-agent systems where each agent manages its own memory.  \n",
    "- **Explainable** â†’ Summaries provide interpretable, inspectable history snapshots.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ’¬ In short: this hybrid design teaches your agent to *think like a human* â€”\n",
    "> remember the essence of what was said, forget unnecessary detail, and stay focused on what matters right now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d9502-089e-45b5-b3e4-c61426ccaf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSummarizingTurnTrimmer(SlidingWindowConversationManager):\n",
    "    def __init__(self, model: WriterModel, context_limit: int = 8, keep_last_n_turns: int = 3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert keep_last_n_turns <= context_limit\n",
    "        self.model = model\n",
    "        self.context_limit = context_limit\n",
    "        self.keep_last_n_turns = keep_last_n_turns\n",
    "\n",
    "    def _is_user_msg(self, item: Dict[str, Any]) -> bool:\n",
    "        return item.get(\"role\") == \"user\"\n",
    "\n",
    "    def _count_turns(self, messages: List[Dict[str, Any]]) -> int:\n",
    "        return sum(1 for m in messages if self._is_user_msg(m))\n",
    "\n",
    "    def _trim_to_last_turns(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        count = 0\n",
    "        start_idx = 0\n",
    "        for i in range(len(messages) - 1, -1, -1):\n",
    "            if self._is_user_msg(messages[i]):\n",
    "                count += 1\n",
    "                if count == self.keep_last_n_turns:\n",
    "                    start_idx = i\n",
    "                    break\n",
    "        return messages[start_idx:]\n",
    "\n",
    "    def _summarize_old_history(self, old_messages: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Use the model to summarize earlier context.\"\"\"\n",
    "        if not old_messages:\n",
    "            return {\"role\": \"assistant\", \"content\": [{\"text\": \"(no previous context)\"}]}\n",
    "    \n",
    "        # Create a temporary summarizer agent using the same model\n",
    "        summarizer = Agent(\n",
    "            model=self.model,\n",
    "            system_prompt=\"You are a concise assistant that summarizes prior conversation context for continuity.\"\n",
    "        )\n",
    "    \n",
    "        # Construct the summarization prompt\n",
    "        prompt = \"Summarize the conversation we had so far briefly but keep all important details.\"\n",
    "    \n",
    "        # Run the summarizer agent to generate the summary\n",
    "        response = summarizer(prompt + \"\\n\\n\" + json.dumps(old_messages, indent=2))\n",
    "    \n",
    "        # Extract the output safely\n",
    "        summary_text = getattr(response, \"output_text\", str(response))\n",
    "        return {\"role\": \"assistant\", \"content\": [{\"text\": summary_text}]}\n",
    "\n",
    "\n",
    "    def select_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Proactively trim and summarize when exceeding context_limit.\"\"\"\n",
    "        if self._count_turns(messages) <= self.context_limit:\n",
    "            return messages\n",
    "\n",
    "        trimmed = self._trim_to_last_turns(messages)\n",
    "        old_msgs = messages[: len(messages) - len(trimmed)]\n",
    "\n",
    "        synthetic_user = {\"role\": \"user\", \"content\": [{\"text\": \"Summarize the conversation we had so far.\"}]}\n",
    "        synthetic_assistant = self._summarize_old_history(old_msgs)\n",
    "\n",
    "        # Replace old history with summary + last turns\n",
    "        return [synthetic_user, synthetic_assistant] + trimmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af1a5b-445e-430e-bd8e-9cefff284a8b",
   "metadata": {},
   "source": [
    "## Initialize the Hybrid Agent\n",
    "\n",
    "Set up the hybrid agent using the `HybridSummarizingTurnTrimmer` we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4787f61-cfc9-4331-a14c-c394c7b8da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = pathlib.Path(\".strands/sessions\").resolve()\n",
    "model = WriterModel(model_id=\"palmyra-x5\")\n",
    "\n",
    "session = FileSessionManager(session_id=\"hybrid-demo\", storage_dir=storage_dir)\n",
    "\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    name=\"Hybrid Assistant\",\n",
    "    system_prompt=\"Be concise but preserve important context.\",\n",
    "    conversation_manager=HybridSummarizingTurnTrimmer(model, context_limit=6, keep_last_n_turns=2),\n",
    "    session_manager=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986b48d-305e-4538-a69e-9c673da84141",
   "metadata": {},
   "source": [
    "## Running the Hybrid Context Demo\n",
    "\n",
    "Letâ€™s simulate a multi-turn conversation to see the hybrid summarization in action.  \n",
    "As the dialogue progresses, older turns will be summarized once the `context_limit` is exceeded â€” keeping the recent turns verbatim and compressing the rest.  \n",
    "Afterward, weâ€™ll inspect both the **active** and **effective** contexts to confirm how the memory evolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89241ce8-25ef-4d54-b478-87cad1e0ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Hybrid Summarization Demo ===\")\n",
    "\n",
    "queries = [\n",
    "    \"Hi there, Iâ€™m researching the history of financial crises.\",\n",
    "    \"Can you tell me what caused the 2008 global financial crisis?\",\n",
    "    \"How did central banks respond to the crisis?\",\n",
    "    \"What were the long-term policy changes that came from it?\",\n",
    "    \"Now, what are the key similarities between 2008 and the 2023 banking stress events?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n User: {q}\")\n",
    "    agent(q)\n",
    "\n",
    "# --- Inspect current context after summarization ---\n",
    "print(\"\\n Active Context (after summarization):\")\n",
    "for msg in agent.messages:\n",
    "    print(msg)\n",
    "\n",
    "# --- Inspect effective context (trimmed + summarization pair) ---\n",
    "print(\"\\n Effective Context (with synthetic summarization pair):\")\n",
    "effective = agent.conversation_manager.select_messages(agent.messages)\n",
    "for msg in effective:\n",
    "    print(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
